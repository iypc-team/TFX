{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a745ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: \u001b[1;94m2.8.2\u001b[0m\n",
      "New files...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from IPython.display import clear_output\n",
    "import json, os\n",
    "from os.path import *\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'\n",
    "from time import perf_counter\n",
    "from BashColors import C\n",
    "from TarfileFunctions import *\n",
    "from CV2_Utils_2 import *\n",
    "\n",
    "# import ImageClassifierDataLoader as icd\n",
    "# from ImageClassifierDataLoader import *\n",
    "\n",
    "import tensorflow as tf \n",
    "print(f'tensorflow version: {C.BIBlue}{tf.__version__}{C.ColorOff}')\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, initializers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "from os.path import exists, join\n",
    "\n",
    "contentPath = os.getcwd()\n",
    "cv2Path=join(contentPath, 'CV2Images')\n",
    "genPath=join(contentPath, 'DataGenerator')\n",
    "testPath=join(contentPath, 'images')\n",
    "checkpointPath = join(contentPath, 'CheckPoints')\n",
    "\n",
    "if not exists(genPath):\n",
    "    tff.extractTarfiles('DataGenerator5.tar.gz')\n",
    "if not exists(testPath):\n",
    "    tff.extractTarfiles('images.tar.gz')\n",
    "    \n",
    "initialGlobList:list\n",
    "with open(\"initialGlobList.json\", 'r') as f:\n",
    "    initialGlobList = json.load(f)\n",
    "    \n",
    "def listNewFiles(initial=initialGlobList, delete=False):\n",
    "    currentFilesGlob=glob.glob('**', recursive=False)\n",
    "    print(f'New files...')\n",
    "    if len(initialGlobList) == len(currentFilesGlob):\n",
    "        print(f'{C.BIRed}No new files.\\nNothing to see here.')\n",
    "    for fil in currentFilesGlob:\n",
    "        if not fil in initial:\n",
    "            if isdir(fil):\n",
    "                print(f'{C.BIBlue}{fil}')\n",
    "                if delete:\n",
    "                    shutil.rmtree(fil)\n",
    "            elif isfile(fil):\n",
    "                print(f'{C.ColorOff}{fil}')\n",
    "                if delete:\n",
    "                    os.remove(fil)\n",
    "listNewFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4896a413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "earlyStop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='accuracy', min_delta=0, patience=2, verbose=1,\n",
    "    mode='auto', baseline=None, restore_best_weights=True,\n",
    ")\n",
    "checkpoints = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpointPath,\n",
    "    monitor='accuracy', verbose=1, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', save_freq='epoch',\n",
    "    options=None\n",
    ")\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90be910c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 844 files belonging to 3 classes.\n",
      "Using 676 files for training.\n",
      "\n",
      "Found 844 files belonging to 3 classes.\n",
      "Using 168 files for validation.\n",
      "\n",
      "Found 4 files belonging to 1 classes.\n",
      "\n",
      "TRAIN_STEPS: 22\n",
      "VAL_STEPS: 6\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE=(224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "train_ds = image_dataset_from_directory(\n",
    "    genPath,\n",
    "    color_mode='rgb',\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=456,\n",
    "    image_size=(224, 224),\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE)\n",
    "print()\n",
    "\n",
    "validation_ds = image_dataset_from_directory(\n",
    "    genPath,\n",
    "    color_mode='rgb',\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=456,\n",
    "    image_size=(224, 224),\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE)\n",
    "print()\n",
    "\n",
    "test_ds = image_dataset_from_directory(\n",
    "    testPath,\n",
    "    color_mode='rgb',\n",
    "    image_size=(224, 224),\n",
    "    shuffle=False,\n",
    "    seed=456,\n",
    "    batch_size = 1)\n",
    "# clear_output()\n",
    "\n",
    "TRAIN_STEPS = train_ds.cardinality().numpy() # 676 // BATCH_SIZE\n",
    "print('\\nTRAIN_STEPS:', TRAIN_STEPS)\n",
    "VAL_STEPS = validation_ds.cardinality().numpy() # 168 // BATCH_SIZE\n",
    "print('VAL_STEPS:', VAL_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26e2ac0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Defcon4_Sequential_V8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 111, 111, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 16)      4624      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 64)        9280      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 26, 26, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 43264)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 43264)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                2768960   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               33280     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,817,040\n",
      "Trainable params: 2,817,040\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential(name='Defcon4_Sequential_V8')\n",
    "model.add(Conv2D(\n",
    "    32, (3,3), activation='relu', input_shape=(224,224,3),\n",
    "    kernel_initializer=initializers.RandomNormal(stddev=0.01),\n",
    "    bias_initializer=initializers.Zeros()\n",
    "))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "          \n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "\n",
    "# model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "# model.add(MaxPooling2D(2,2))\n",
    "# model.add(Conv2D(256, (3,3), activation='relu'))\n",
    "# model.add(MaxPooling2D(2,2))\n",
    "# model.add(Conv2D(512, (3,3), activation='relu'))\n",
    "# model.add(MaxPooling2D(2,2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu')) # hidden layer\n",
    "\n",
    "model.add(Dense(512, activation='softmax'))\n",
    "\n",
    "model.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76273028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False\n",
    "myOptimizer = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.005, rho=0.9, momentum=0.0, epsilon=1e-07,\n",
    "    centered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5f75fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer = myOptimizer,\n",
    "              metrics=['accuracy'])\n",
    "model.built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69961174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 88.7920 - accuracy: 0.6198\n",
      "Epoch 1: accuracy improved from -inf to 0.61982, saving model to /home/jovyan/CheckPoints\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/CheckPoints/assets\n",
      "22/22 [==============================] - 102s 5s/step - loss: 88.7920 - accuracy: 0.6198 - val_loss: 0.3210 - val_accuracy: 0.8988\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.9009\n",
      "Epoch 2: accuracy improved from 0.61982 to 0.90089, saving model to /home/jovyan/CheckPoints\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/CheckPoints/assets\n",
      "22/22 [==============================] - 95s 4s/step - loss: 0.2662 - accuracy: 0.9009 - val_loss: 0.3696 - val_accuracy: 0.9048\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5084 - accuracy: 0.8876\n",
      "Epoch 3: accuracy did not improve from 0.90089\n",
      "22/22 [==============================] - 94s 4s/step - loss: 0.5084 - accuracy: 0.8876 - val_loss: 2.2819 - val_accuracy: 0.4286\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5089 - accuracy: 0.8994Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 4: accuracy did not improve from 0.90089\n",
      "22/22 [==============================] - 89s 4s/step - loss: 0.5089 - accuracy: 0.8994 - val_loss: 0.3132 - val_accuracy: 0.8869\n",
      "Epoch 4: early stopping\n",
      "\n",
      "completed: 6.0 minutes 20.0 second(s)\n"
     ]
    }
   ],
   "source": [
    "startTimer=perf_counter()\n",
    "history  =  model.fit(\n",
    "    x = train_ds,\n",
    "    y = None,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = 10,\n",
    "    verbose = 'auto',\n",
    "    callbacks = [earlyStop, checkpoints],\n",
    "    validation_split = 0.0,\n",
    "    validation_data = validation_ds,\n",
    "    shuffle = False,\n",
    "    class_weight = None,\n",
    "    sample_weight = None,\n",
    "    initial_epoch = 0,\n",
    "    steps_per_epoch = TRAIN_STEPS,\n",
    "    validation_steps = VAL_STEPS,\n",
    "    validation_batch_size = BATCH_SIZE,\n",
    "    validation_freq = 1,\n",
    "    max_queue_size = 6,\n",
    "    workers = 4,\n",
    "    use_multiprocessing = True,\n",
    ")\n",
    "finishTimer=perf_counter()\n",
    "print()\n",
    "cvu.printTime(startTimer, finishTimer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b49f018c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function plotShowTFLiteImagePredictions in module __main__:\n",
      "\n",
      "plotShowTFLiteImagePredictions()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def plotShowTFLiteImagePredictions():\n",
    "    ''' '''\n",
    "    # https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/image_classifier/DataLoader\n",
    "    # A helper function that returns 'red'/'black' depending on if its two input\n",
    "    # parameter matches or not.\n",
    "    def get_label_color(val1, val2):\n",
    "        ''' '''\n",
    "        if val1 == val2:\n",
    "            return 'black'\n",
    "        else:\n",
    "            return 'red'\n",
    "\n",
    "    # Then plot 100 test images and their predicted labels.\n",
    "    # If a prediction result is different from the label provided label in \"test\"\n",
    "    # dataset, we will highlight it in red color.\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    predicts = model.predict_top_k(test_data)\n",
    "    for i, (image, label) in enumerate(test_data.gen_dataset().unbatch().take(10)):\n",
    "        ax = plt.subplot(10, 10, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(image.numpy(), cmap=plt.cm.gray)\n",
    "        predict_label = predicts[i][0][0]\n",
    "        color = get_label_color(\n",
    "            predict_label, test_data.index_to_label[label.numpy()])\n",
    "        ax.xaxis.label.set_color(color)\n",
    "        plt.xlabel('Predicted: %s' % predict_label)\n",
    "    plt.show()\n",
    "help(plotShowTFLiteImagePredictions)\n",
    "# plotShowTFLiteImagePredictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f1b35ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "for i, (image, label) in enumerate(test_ds.unbatch().take(1)):\n",
    "  plt.subplot(3, 3, i+1)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.grid(False)\n",
    "  plt.imshow(image.numpy(), cmap=plt.cm.gray)\n",
    "  # plt.xlabel(data.index_to_label[label.numpy()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff6a4a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved to: /home/jovyan/Defcon4_Sequential_V8_dnn.h5\n"
     ]
    }
   ],
   "source": [
    "modelName = model.name + '_dnn.h5'\n",
    "modelSavePath = join(contentPath, modelName)\n",
    "print(f'model saved to: {modelSavePath}')\n",
    "model.save(modelSavePath, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dde86014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newModelName: \u001b[1;95mDefcon4_Sequential_V8.tflite\u001b[0m\n",
      "saved: \u001b[1;92mDefcon4_Sequential_V8.tflite\n"
     ]
    }
   ],
   "source": [
    "newModelName = model.name + '.tflite'\n",
    "# modelSavePath = join(contentPath, modelName)\n",
    "print(f'newModelName: {C.BIPurple}{newModelName}{C.ColorOff}')\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(checkpointPath) # path to the SavedModel directory\n",
    "converter.target_spec.supported_ops = [\n",
    "  tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n",
    "]\n",
    "converter.optimizations=[tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open(newModelName, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "if exists(newModelName):\n",
    "    print(f'saved: {C.BIGreen}{newModelName}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f92fb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New files...\n",
      "\u001b[0mDefcon4_Sequential_V8.tflite\n",
      "\u001b[1;94mCheckPoints\n",
      "\u001b[0mDefcon4_Sequential_V8_dnn.h5\n"
     ]
    }
   ],
   "source": [
    "listNewFiles()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
